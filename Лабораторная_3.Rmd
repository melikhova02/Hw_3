---
title: "Лабораторная_3"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
  word_document: default
---

### Параметрические классификаторы для бинарной зависимой переменной (Y)

В данной задаче выполняются следующие пункты:   
 
* построение модели логистической регрессии;
* построение модели линейного дискриминантного анализа (LDA);
* построение модели квадратичного дискриминантного анализа (QDA);
* построение ROC-кривой.

*Модели*: логистическая регрессия, LDA, QDA.  
*Данные*: сгенерированный набор случаев диабета у женщин индейского племени Пима PimaIndiansDiabetes{mlbench}.В наборе 768 наблюдений и 9 показателей:

* pregnant - количество беременностей;
* glucose	- значения глюкозотолерантного теста;
* pressure -	 диастолическое давление;
* triceps	- Толщина створки кожи трицепсов;
* insulin	- инсулин в сыворотке крови;
* mass - индекс массы тела;
* pedigree	- функция родословной диабета;
* age	- возраст;
* diabetes - зависимая переменная,тест на диабет.(pos – наличие признака, neg – отсутствие)

# Пакеты:

```{r packages}
library('mlbench')
library('GGally')
library('MASS')
```

# Зададим ядро генератора случайных чисел и объём обучающей выборки.

```{r seed}
my.seed <- 123
train.percent <- 0.75
```

## Исходные данные: набор PimaIndiansDiabetes

```{r data}
data(PimaIndiansDiabetes)
head(PimaIndiansDiabetes)
str(PimaIndiansDiabetes)
ggpairs(PimaIndiansDiabetes)
```

## Отбираем наблюдения в обучающую выборку

```{r intrain}
set.seed(my.seed)
inTrain <- sample(seq_along(PimaIndiansDiabetes$diabetes),
                  nrow(PimaIndiansDiabetes) * train.percent)
df <- PimaIndiansDiabetes[inTrain, ] 
inTest <- -inTrain
df1 <- PimaIndiansDiabetes[inTest, ] 
Факт <- df$diabetes
```

### Строим модели, чтобы спрогнозировать diabetes

##Логистическая регрессия

Из модели, изначально содержащей все исходные объяснящие переменные, были исключены следующие незначимые переменные: triceps, age. Таким образом получаем, что в окончательной модели параметры логистической регрессии значимы с вероятностью 0.95.

```{r logit}
#ОБУЧ
Факт <- df$diabetes
model.logit <- glm(diabetes ~ pregnant+glucose+pressure+insulin+mass+pedigree, 
                   data = df, 
                   family = 'binomial')    
    
summary(model.logit)

# прогноз: вероятности принадлежности классу 'pos' (диабет)
p.logit <- predict(model.logit, df, type = 'response')
    
Прогноз <- factor(ifelse(p.logit > 0.5, 2, 1),
                  levels = c(1,2),
                  labels = c('neg','pos')) 
    
    

# матрица неточностей
conf.m <- table(Факт, Прогноз)
conf.m

# чувствительность
conf.m[2, 2] / sum(conf.m[2, ])
# специфичность
conf.m[1, 1] / sum(conf.m[1, ])
# верность
sum(diag(conf.m)) / sum(conf.m)  


#ТЕСТ
# фактические значения на обучающей выборке
Факт1 <- df1$diabetes


# Строим модели, чтобы спрогнозировать diabetes ---------------------------------

# Логистическая регрессия ======================================================
model.logit <- glm(diabetes ~ pregnant+glucose+pressure+insulin+mass+pedigree, 
                   data = df1, 
                   family = 'binomial')    

summary(model.logit)

# прогноз: вероятности принадлежности классу 'pos' (диабет)
p.logit <- predict(model.logit, df1, type = 'response')

Прогноз1 <- factor(ifelse(p.logit > 0.5, 2, 1),
                  levels = c(1,2),
                  labels = c('neg','pos')) 



# матрица неточностей
conf.m <- table(Факт1, Прогноз1)
conf.m

# чувствительность
conf.m[2, 2] / sum(conf.m[2, ])
# специфичность
conf.m[1, 1] / sum(conf.m[1, ])
# верность
sum(diag(conf.m)) / sum(conf.m)

```

## LDA

Отчёт по модели LDA содержит три раздела: априарные вероятности классов (Prior probabilities of groups), групповые средние объясняющих переменных (Group means) и коэффициенты линейной разделяющей границы (Coefficients of linear discriminants).
У этой модели чувствительность аналогичная. Оценим модели LDA по обучающей и тестовой выборке. 

```{r lda}
#ОБУЧ
model.lda <- lda(diabetes ~ pregnant+glucose+pressure+insulin+mass+pedigree, 
                 data = df)
    
model.lda

# прогноз: вероятности принадлежности классу 'pos' (диабет)
p.lda <- predict(model.lda, df, 
                 type = 'response')
Прогноз <- factor(ifelse(p.lda$posterior[, 'pos'] > 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c('neg', 'pos'))

# матрица неточностей
conf.m <- table(Факт, Прогноз)
conf.m

# чувствительность
conf.m[2, 2] / sum(conf.m[2, ])
# специфичность
conf.m[1, 1] / sum(conf.m[1, ])
# верность
sum(diag(conf.m)) / sum(conf.m)




#ТЕСТ
model.lda1 <- lda(diabetes ~ pregnant+glucose+pressure+insulin+mass+pedigree, 
                 data = df1)

model.lda1

# прогноз: вероятности принадлежности классу 'pos' (диабет)
p.lda1 <- predict(model.lda1, df1, 
                 type = 'response')
Прогноз1 <- factor(ifelse(p.lda1$posterior[, 'pos'] > 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c('neg', 'pos'))

# матрица неточностей
conf.m <- table(Факт1, Прогноз1)
conf.m

# чувствительность
conf.m[2, 2] / sum(conf.m[2, ])
# специфичность
conf.m[1, 1] / sum(conf.m[1, ])
# верность
sum(diag(conf.m)) / sum(conf.m)

```

Чувствительность на тестовой выборке значительно выше, чем на обучающей.

## QDA

Оценим модели QDA по обучающей и тестовой выборке. 

```{r qda}
#ОБУЧ
model.qda <- qda(diabetes ~ pregnant+glucose+pressure+insulin+mass+pedigree, data = df)
    
model.qda

# прогноз: вероятности принадлежности классу 'pos' (диабет)
p.qda <- predict(model.qda, df, type = 'response')
Прогноз <- factor(ifelse(p.qda$posterior[, 'pos'] > 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c('neg', 'pos'))

# матрица неточностей
conf.m <- table(Факт, Прогноз)
conf.m

# чувствительность
conf.m[2, 2] / sum(conf.m[2, ])
# специфичность
conf.m[1, 1] / sum(conf.m[1, ])
# верность
sum(diag(conf.m)) / sum(conf.m)

#ТЕСТ
model.qda1 <- qda(diabetes ~ pregnant+glucose+pressure+insulin+mass+pedigree, data = df1)

model.qda1

# прогноз: вероятности принадлежности классу 'pos' (диабет)
p.qda1 <- predict(model.qda1, df1, type = 'response')
Прогноз1 <- factor(ifelse(p.qda1$posterior[, 'pos'] > 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c('neg', 'pos'))

# матрица неточностей
conf.m <- table(Факт1, Прогноз1)
conf.m

# чувствительность
conf.m[2, 2] / sum(conf.m[2, ])
# специфичность
conf.m[1, 1] / sum(conf.m[1, ])
# верность
sum(diag(conf.m)) / sum(conf.m)

```

Чувствительность на тестовой выборке значительно выше, чем на обучающей.

##ROC-кривые (1)

Построим две ROC-кривые на одних осях и сравним качества прогноза моделей на обучающей выборке.

```{r roc1}
# считаем 1-SPC и TPR для всех вариантов границы отсечения
x <- NULL    # для (1 - SPC)
y <- NULL    # для TPR

# заготовка под матрицу неточностей
tbl <- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl) <- c('fact.neg', 'fact.pos')
colnames(tbl) <- c('predict.neg', 'predict.pos')

# цикл по вероятностям отсечения
for (p in seq(0, 1, length = 501)){
    # прогноз
    Прогноз <- factor(ifelse(p.qda$posterior[, 'pos'] > p, 
                               2, 1),
                        levels = c(1, 2),
                        labels = c('neg', 'pos'))
    # фрейм со сравнением факта и прогноза
    df.compare <- data.frame(Факт = Факт, Прогноз = Прогноз)
    # заполняем матрицу неточностей
    # TN
    tbl[1, 1] <- nrow(df.compare[df.compare$Факт == 'neg' & df.compare$Прогноз == 'neg', ])
        
    # TP
    tbl[2, 2] <- nrow(df.compare[df.compare$Факт == 'pos' & df.compare$Прогноз == 'pos', ])
        
    # FP
    tbl[1, 2] <- nrow(df.compare[df.compare$Факт == 'neg' & df.compare$Прогноз == 'pos', ])
    
    # FN
    tbl[2, 1] <- nrow(df.compare[df.compare$Факт == 'pos' & df.compare$Прогноз == 'neg', ])
        
    
    # считаем характеристики
    TPR <- tbl[2, 2] / sum(tbl[2, ])
    y <- c(y, TPR)
    SPC <- tbl[1, 1] / sum(tbl[1, ])
    x <- c(x, 1 - SPC)
}
x1 <- NULL    # для (1 - SPC)
y1 <- NULL    # для TPR

# заготовка под матрицу неточностей
tbl1 <- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl1) <- c('fact.neg', 'fact.pos')
colnames(tbl1) <- c('predict.neg', 'predict.pos')

# цикл по вероятностям отсечения
for (p in seq(0, 1, length = 501)){
  # прогноз
  Прогноз <- factor(ifelse(p.lda$posterior[, 'pos'] > p, 
                           2, 1),
                    levels = c(1, 2),
                    labels = c('neg', 'pos'))
  # фрейм со сравнением факта и прогноза
  df.compare <- data.frame(Факт = Факт, Прогноз = Прогноз)
  # заполняем матрицу неточностей
  # TN
  tbl1[1, 1] <- nrow(df.compare[df.compare$Факт == 'neg' & df.compare$Прогноз == 'neg', ])
  
  # TP
  tbl1[2, 2] <- nrow(df.compare[df.compare$Факт == 'pos' & df.compare$Прогноз == 'pos', ])
  
  # FP
  tbl1[1, 2] <- nrow(df.compare[df.compare$Факт == 'neg' & df.compare$Прогноз == 'pos', ])
  
  # FN
  tbl1[2, 1] <- nrow(df.compare[df.compare$Факт == 'pos' & df.compare$Прогноз == 'neg', ])
  
  
  # считаем характеристики
  TPR1 <- tbl1[2, 2] / sum(tbl1[2, ])
  y1 <- c(y1, TPR1)
  SPC1 <- tbl1[1, 1] / sum(tbl1[1, ])
  x1 <- c(x1, 1 - SPC1)
}

# строим ROC-кривую
par(mar = c(5, 5, 1, 1))
# кривая
plot(x, y, 
     type = 'l', col = 'blue', lwd = 3,
     xlab = '(1 - SPC)', ylab = 'TPR', 
     xlim = c(0, 1), ylim = c(0, 1))
lines(x1,y1, type = 'l', col = 'red', lwd = 3,
      xlim = c(0, 1), ylim = c(0, 1))
# прямая случайного классификатора
abline(a = 0, b = 1, lty = 3, lwd = 2)
```


##ROC-кривые (2)

Построим две ROC-кривые на одних осях и сравним качества прогноза моделей на тестовой выборке.

```{r roc2}
# считаем 1-SPC и TPR для всех вариантов границы отсечения
x <- NULL    # для (1 - SPC)
y <- NULL    # для TPR

# заготовка под матрицу неточностей
tbl <- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl) <- c('fact.neg', 'fact.pos')
colnames(tbl) <- c('predict.neg', 'predict.pos')

# цикл по вероятностям отсечения
for (p in seq(0, 1, length = 501)){
    # прогноз
    Прогноз1 <- factor(ifelse(p.qda1$posterior[, 'pos'] > p, 
                               2, 1),
                        levels = c(1, 2),
                        labels = c('neg', 'pos'))
    # фрейм со сравнением факта и прогноза
    df1.compare <- data.frame(Факт1 = Факт1, Прогноз1 = Прогноз1)
    # заполняем матрицу неточностей
    # TN
    tbl[1, 1] <- nrow(df1.compare[df1.compare$Факт1 == 'neg' & df1.compare$Прогноз1 == 'neg', ])
        
    # TP
    tbl[2, 2] <- nrow(df1.compare[df1.compare$Факт1 == 'pos' & df1.compare$Прогноз1 == 'pos', ])
        
    # FP
    tbl[1, 2] <- nrow(df1.compare[df1.compare$Факт1 == 'neg' & df1.compare$Прогноз1 == 'pos', ])
    
    # FN
    tbl[2, 1] <- nrow(df1.compare[df1.compare$Факт1 == 'pos' & df1.compare$Прогноз1 == 'neg', ])
        
    
    # считаем характеристики
    TPR <- tbl[2, 2] / sum(tbl[2, ])
    y <- c(y, TPR)
    SPC <- tbl[1, 1] / sum(tbl[1, ])
    x <- c(x, 1 - SPC)
}
x1 <- NULL    # для (1 - SPC)
y1 <- NULL    # для TPR

# заготовка под матрицу неточностей
tbl1 <- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl1) <- c('fact.neg', 'fact.pos')
colnames(tbl1) <- c('predict.neg', 'predict.pos')

# цикл по вероятностям отсечения
for (p in seq(0, 1, length = 501)){
  # прогноз
  Прогноз1 <- factor(ifelse(p.lda1$posterior[, 'pos'] > p, 
                           2, 1),
                    levels = c(1, 2),
                    labels = c('neg', 'pos'))
  # фрейм со сравнением факта и прогноза
  df1.compare <- data.frame(Факт1 = Факт1, Прогноз1 = Прогноз1)
  # заполняем матрицу неточностей
  # TN
  tbl1[1, 1] <- nrow(df1.compare[df1.compare$Факт1 == 'neg' & df1.compare$Прогноз1 == 'neg', ])
  
  # TP
  tbl1[2, 2] <- nrow(df1.compare[df1.compare$Факт1 == 'pos' & df1.compare$Прогноз1 == 'pos', ])
  
  # FP
  tbl1[1, 2] <- nrow(df1.compare[df1.compare$Факт1 == 'neg' & df1.compare$Прогноз1 == 'pos', ])
  
  # FN
  tbl1[2, 1] <- nrow(df1.compare[df1.compare$Факт1 == 'pos' & df1.compare$Прогноз1 == 'neg', ])
  
  
  # считаем характеристики
  TPR1 <- tbl1[2, 2] / sum(tbl1[2, ])
  y1 <- c(y1, TPR1)
  SPC1 <- tbl1[1, 1] / sum(tbl1[1, ])
  x1 <- c(x1, 1 - SPC1)
}

# строим ROC-кривую
par(mar = c(5, 5, 1, 1))
# кривая
plot(x, y, 
     type = 'l', col = 'blue', lwd = 3,
     xlab = '(1 - SPC)', ylab = 'TPR', 
     xlim = c(0, 1), ylim = c(0, 1))
lines(x1,y1, type = 'l', col = 'red', lwd = 3,
      xlim = c(0, 1), ylim = c(0, 1))
# прямая случайного классификатора
abline(a = 0, b = 1, lty = 3, lwd = 2)
```

Чем ближе кривая к верхнему левому углу, тем выше предсказательная способность модели. Наоборот, чем меньше изгиб кривой и чем ближе она расположена к диагональной прямой, тем менее эффективна модель. Таким образом, можно сделать вывод о том, что ROC-кривые, построенные на обучающей и тестовой выборках  для LDA(красные линии) оказались лучше, чем для QDA.